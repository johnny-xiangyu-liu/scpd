{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa990bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision import tv_tensors  # we'll describe this a bit later, bare with us\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from pathlib import Path\n",
    "\n",
    "from torchview import draw_graph\n",
    "from pathlib import Path\n",
    "\n",
    "import constants\n",
    "import dataset\n",
    "import util\n",
    "import json\n",
    "import pandas as pd\n",
    "import models \n",
    "from models import VQANet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import traceback\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():      \n",
    "    device = 'mps'                         \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print('using device:', device)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2281ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498b42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def show(imgs):\n",
    "#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "#     for i, img in enumerate(imgs):\n",
    "#         img = T.ToPILImage()(img.to('cpu'))\n",
    "#         axs[0, i].imshow(np.asarray(img))\n",
    "#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c7e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(constants.CAPTION_TRAIN, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "#     print(data[\"annotations\"][0])\n",
    "\n",
    "# with open(constants.VQA_OPEN_ENDED_QUESTION_TRAIN, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "#     print(data[\"questions\"][0])\n",
    "\n",
    "# with open(constants.VQA_OPEN_ENDED_ANSWER_TRAIN, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "#     print(data[\"annotations\"][0])\n",
    "    \n",
    "# with open(constants.CAPTION_VAL, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "\n",
    "# with open(constants.VQA_OPEN_ENDED_QUESTION_VAL, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "\n",
    "# with open(constants.VQA_OPEN_ENDED_ANSWER_VAL, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data.keys())\n",
    "\n",
    "#dataset.load(constants.VQA_OPEN_ENDED_QUESTION_TRAIN, ['image_id', 'id', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6beedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/train' if necessary\n",
      "Found annotations at '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/raw/instances_train2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'validation' to '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/validation' if necessary\n",
      "Found annotations at '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'test' to '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/test' if necessary\n",
      "Found test info at '/Users/xiangyuliu/sources/fiftyone_dataset_zoo/coco-2017/raw/image_info_test2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Loading existing dataset 'coco-2017-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "train = dataset.Coco()\n",
    "val = dataset.Coco(\"validation\")\n",
    "test = dataset.Coco(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14b6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118287\n",
      "591753\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(train.captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e46628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False: # debug\n",
    "    img = train.__getitem__(1)\n",
    "    print(img)\n",
    "    print(img.image_id)\n",
    "    print(img.image_path)\n",
    "\n",
    "    print(\">>>>\")\n",
    "    print(img.captions())\n",
    "\n",
    "    print(\">>>>\")\n",
    "    print(img.qa())\n",
    "    print(\"shape\", img.image_tensor().shape)\n",
    "\n",
    "    show([img.image_tensor()])\n",
    "\n",
    "#plt.imshow(  img.image_tensor().permute(1, 2, 0)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf4fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer  = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# Add the Q and A token as special token\n",
    "tokenizer.add_special_tokens(constants.QA_TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceaf2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagedata import get_image\n",
    "\n",
    "def path_to_image(paths, device):\n",
    "    result = []\n",
    "    for p in paths:\n",
    "        result.append(get_image(path, device))\n",
    "    image = torch.stack(result, dim = 0).to(device)\n",
    "    print(\"image shape\", image.shape)\n",
    "    return image\n",
    "    \n",
    "def collate_fn2(batch):\n",
    "    result = {}\n",
    "    \n",
    "    result['image_ids'] = []\n",
    "    result['image_paths'] = []\n",
    "    result['c2i'] = [] # index for images for a given caption. same len as 'caption'\n",
    "    result['qa2i'] = [] # index of corresponding image for a given qa. same len as 'qa'\n",
    "    result['q_id'] = [] # id of the questions 'qa'\n",
    "    \n",
    "    \n",
    "    target  = [] # the corresponding target for the qa.\n",
    "    raw_captions = []  # plain text \n",
    "    raw_qa = []   # plain text\n",
    "    raw_qids = []   # question ids\n",
    "    for idx, data in enumerate(batch):\n",
    "        result['image_ids'].append(data.image_id)\n",
    "        result['image_paths'].append(data.image_path)\n",
    "        caption_list = data.captions()\n",
    "        if caption_list is not None:\n",
    "            raw_captions += caption_list\n",
    "            for c in range(len(caption_list)):\n",
    "                result['c2i'].append(idx)\n",
    "        \n",
    "        qa_list = data.qa()\n",
    "        q_id_list = data.qids()\n",
    "        if qa_list is not None:\n",
    "            raw_qa += qa_list\n",
    "            raw_qids += q_id_list\n",
    "            for c in range(len(qa_list)):\n",
    "                result['qa2i'].append(idx)\n",
    "    #print(\"raw_cap\", len(raw_captions))\n",
    "    #print(\"raw_qa\", len(raw_qa))\n",
    "    \n",
    "    result['raw_cap'] = raw_captions\n",
    "    result['captions'] = None if len(raw_captions) == 0 else \\\n",
    "                                tokenizer(raw_captions, padding='max_length',truncation=True,  max_length=48, return_tensors=\"pt\").to(device)\n",
    "#    print('captions:', result['captions'])\n",
    "    result['raw_qa'] = raw_qa\n",
    "    result['qids'] = raw_qids\n",
    "    if len(raw_qa) != 0:\n",
    "#        print(\"raw_qa:\", raw_qa)\n",
    "        result['qa'] =  tokenizer(raw_qa, padding='max_length', truncation=True, max_length=48, return_tensors=\"pt\")['input_ids'].to(device, dtype=torch.int64)\n",
    "#        print('qa:', result['qa'])\n",
    "\n",
    "        end_padding = torch.broadcast_to(torch.zeros(1), (result['qa'].shape[0], 1)).to(device, dtype=torch.int64)\n",
    "        #print(end_padding.shape)\n",
    "        # return a shape {seq, batch}\n",
    "        target = torch.column_stack((result['qa'][:, 1:], end_padding)).transpose(0, 1)\n",
    "    else:\n",
    "        result['qa'] = None\n",
    "        target = None\n",
    "    return result, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b5181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "fn = collate_fn2 \n",
    "shuffle = False  # True\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=shuffle, collate_fn=fn)\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size, shuffle=shuffle, collate_fn=fn)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=shuffle, collate_fn=fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee57bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check(dataloader, size=5):\n",
    "    it = iter(dataloader)\n",
    "    for _ in range(size):\n",
    "        x, target= next(it)\n",
    "        print(x)\n",
    "        show(x[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4871ac4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spot_check(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9175e997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spot_check(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f50ca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_fn = nn.CrossEntropyLoss( reduction='none')\n",
    "cos_fn = nn.CosineSimilarity(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15f724e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# out = model(x, device)\n",
    "# image_embedding, captions_embeddings, output_logits = out\n",
    "# print(captions_embeddings.shape)\n",
    "# a = output_logits.reshape(-1, len(tokenizer))\n",
    "# b = target.reshape(-1)\n",
    "# print(\"a\", a.shape, a)\n",
    "# print(\"b\", b.shape, b)\n",
    "\n",
    "# ce_loss = ce_fn(a, b)\n",
    "# print(ce_loss.shape)\n",
    "# N = len(x['images'])\n",
    "# M = len(x['qa2i'])\n",
    "# ce = ce_loss.reshape(-1, M).transpose(0, 1)\n",
    "# print(ce.shape)\n",
    "# print(ce)\n",
    "# per_qa  = torch.mean(ce, axis = 1)\n",
    "# print(per_qa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b64065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blown = models.blow_to(image_embedding, result['c2i'])\n",
    "# print(image_embedding.shape)\n",
    "# print(image_embedding)\n",
    "# print(blown.shape)\n",
    "# print(blown)\n",
    "# print(\"captions_embedding:\", captions_embeddings.shape)\n",
    "# print(result['c2i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ee0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(blown)\n",
    "# print(captions_embeddings)\n",
    "# cos= nn.CosineSimilarity(dim = 0)\n",
    "# print(cos(blown[1], captions_embeddings[1]))\n",
    "\n",
    "# per_caption_loss = cos_fn(blown, captions_embeddings)\n",
    "# print(per_caption_loss)\n",
    "# per_image_caption_loss = cal_average(len(result['images']), per_caption_loss, result['c2i'])\n",
    "# print(per_image_caption_loss.shape)\n",
    "\n",
    "# print(per_image_caption_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ee02034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model( lr, name):\n",
    "    model = VQANet(tokenizer).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    if name is not None:\n",
    "        checkpoint = torch.load(constants.MODEL_OUT_PATH.joinpath(name))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.to(device)\n",
    "    return model, optimizer\n",
    "\n",
    "def save_model(model, optimizer, name):\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()},\n",
    "        constants.MODEL_OUT_PATH.joinpath(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd8b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_average(size, blown_loss, replicas):\n",
    "    result= torch.zeros(size).to(device)\n",
    "    counts = torch.zeros(size).to(device)\n",
    "    for index, val in enumerate(replicas):\n",
    "        result[val] += blown_loss[index]\n",
    "        counts[val] += 1\n",
    "        \n",
    "    for index in range(size):\n",
    "        if counts[index] == 0:\n",
    "            counts[index] = 1  # so that result / counts still makes sense.\n",
    "    #print(\"result\", result)\n",
    "    #print(\"counts:\", counts)\n",
    "    counts = counts.detach()  # we don't need gradient for the counts.\n",
    "    result /= counts\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "789feeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gamma = 0.9\n",
    "DEBUG = False\n",
    "def do_train(model, optimizer, idx, x, target, should_print = False):\n",
    "        N = len(x['image_ids'])\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        if DEBUG:\n",
    "            image_embedding_for_captions, captions_embedding, output_logits = None, None, None\n",
    "        else:\n",
    "            image_embedding_for_captions, captions_embedding, output_logits  = model(x, device)\n",
    "\n",
    "        if output_logits is None and captions_embedding is None:\n",
    "            return torch.tensor(0).to(device)\n",
    "#        image_embedding, captions_embedding, output_logits = None, None, None\n",
    "        per_image_qa_loss = None\n",
    "        per_image_caption_loss = None\n",
    "        \n",
    "        if output_logits is not None:\n",
    "#            print(\"out_logits argmax\", torch.argmax(output_logits.transpose(0,1), axis=2))\n",
    "#            print(\"target\", target)\n",
    "            a = output_logits.reshape(-1, len(tokenizer))\n",
    "\n",
    "            b = target.reshape(-1)\n",
    "#            print(\"a\", a.shape)\n",
    "#            print(\"b\", b.shape)\n",
    "\n",
    "            K = len(x['qa2i'])\n",
    "            # back to (K, seq)\n",
    "            qa_loss = ce_fn(a, b).reshape(-1, K).transpose(0, 1)\n",
    "            #print(\"qa_loss\", qa_loss.shape)\n",
    "            # qa loss, shape of (K) (different images can have diff counts of qas)\n",
    "            per_qa_loss = torch.mean(qa_loss, axis = 1)\n",
    "\n",
    "            # per image qa loss, shape of (N)\n",
    "            per_image_qa_loss = cal_average(N, per_qa_loss, x['qa2i'])\n",
    "            #print(\"per_qa_loss\", per_qa_loss.shape)\n",
    "            #print(\"per_image_qa_loss\", per_image_qa_loss.shape)\n",
    "\n",
    "        if captions_embedding is not None:\n",
    "            # loss per caption, shape of (M) (different images can have diff counts of captions)\n",
    "            per_caption_loss = cos_fn(image_embedding_for_captions, captions_embedding)\n",
    "            # cosine similarity is within [-1, 1] where 1 being similar. \n",
    "            # for loss, we invert it and shift it by 1 to keep the value always positive.\n",
    "            # thus 0 means similar, 2 means completely opposite\n",
    "            # print(\"per_captions_loss:\", per_caption_loss)\n",
    "            per_caption_loss = -per_caption_loss + 1\n",
    "            # print(\"normalized per_captions_loss:\", per_caption_loss)\n",
    "            # per image loss on the caption scale. shape of (N)\n",
    "            per_image_caption_loss = cal_average(N, per_caption_loss, x['c2i'])\n",
    "\n",
    "        total_loss = 0\n",
    "        if per_image_qa_loss is not None:\n",
    "            total_loss += gamma * per_image_caption_loss\n",
    "            \n",
    "        if per_image_qa_loss is not None:\n",
    "            total_loss += per_image_qa_loss\n",
    "\n",
    "        loss = torch.sum(total_loss)\n",
    "\n",
    "        if not DEBUG:\n",
    "            loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        del per_image_caption_loss\n",
    "        del per_image_qa_loss\n",
    "        del x\n",
    "        del total_loss\n",
    "        return loss\n",
    "            \n",
    "def training(run_name, writer, epoches, early_terminate = None, sync_after_every_n = 200, print_every = 100):\n",
    "    lr = 0.1\n",
    "    model, optimizer = reload_model(lr, None)\n",
    "    \n",
    "    model_name = 'train-' + run_name\n",
    "    \n",
    "    model.train()\n",
    "    for epoch_idx in range(epoches):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        print (\"----- Start Epoch %s -----\" % epoch_idx)\n",
    "        epoch_loss = 0\n",
    "        for idx, (x, target) in enumerate(train_dataloader):\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            should_print = print_every is not None and (print_every == 1 or idx % (print_every -1) == 0)\n",
    "            if should_print:\n",
    "                print(\">>>> Batch # \", idx,  x['image_ids'] )\n",
    "            if early_terminate is not None:\n",
    "                if idx > early_terminate - 1:\n",
    "                    print(\"early terminating. at \", idx)\n",
    "                    break;\n",
    "            try:\n",
    "                loss = do_train(model, optimizer, idx, x, target, should_print).detach()\n",
    "                batch_loss = loss.item()\n",
    "            except Exception as e:\n",
    "                print(\">>>> FAILED! Batch # \", idx,  x['image_ids'])\n",
    "                print(f\"current mps allocated memory: {torch.mps.current_allocated_memory()}\")\n",
    "                print(f\"current mps driver allocated memory: {torch.mps.driver_allocated_memory()}\")\n",
    "                traceback.print_exc()\n",
    "                return model;\n",
    "\n",
    "            if sync_after_every_n is not None and (idx + 1) % sync_after_every_n == 0:\n",
    "                print(\"=========== mps sync, gc, and mps empty cache and reload ==========\")\n",
    "                print(f\"current mps allocated memory: {torch.mps.current_allocated_memory()}\")\n",
    "                print(f\"current mps driver allocated memory: {torch.mps.driver_allocated_memory()}\")\n",
    "                name = model_name + f\"-epoch-{epoch_idx}-batch-{idx}\"\n",
    "                save_model(model, optimizer, name)\n",
    "                del model\n",
    "                del loss\n",
    "                del optimizer\n",
    "                gc.collect()\n",
    "                torch.mps.synchronize()\n",
    "                torch.mps.empty_cache()\n",
    "                torch.mps.synchronize()\n",
    "                print(\"after empyt cache\")\n",
    "                print(f\"current mps allocated memory: {torch.mps.current_allocated_memory()}\")\n",
    "                print(f\"current mps driver allocated memory: {torch.mps.driver_allocated_memory()}\")\n",
    "                model, optimizer = reload_model(lr, name)\n",
    "                model.train()\n",
    "\n",
    "            writer.add_scalar(\"Loss/train Batch Time\", (time.time() - batch_start_time))\n",
    "\n",
    "            if should_print:\n",
    "                print(\"loss:\", batch_loss)\n",
    "                print(\"--- %s Per batch time ---\" % (time.time() - batch_start_time))\n",
    "\n",
    "            writer.add_scalar(f\"Loss/train Per Batch for Epoch {epoch_idx}\", batch_loss, idx)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "        epoch_loss /= len(train_dataloader) if early_terminate is None \\\n",
    "                                            else (early_terminate * train_dataloader.batch_size)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch_idx)\n",
    "\n",
    "        print(f\"---DONE: {epoch_idx} epoch, {(time.time() - epoch_start_time)} seconds, loss {epoch_loss} ---\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0da6ef6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Batch #  522 [82061, 82081, 82083, 82084, 82091, 82106, 82118, 82121, 82122, 82124, 82131, 82135, 82138, 82140, 82142, 82143, 82144, 82147, 82150, 82157, 82174, 82191, 82198, 82201, 82202, 82212, 82216, 82220, 82225, 82228, 82229, 82242]\n",
      "loss: 15.569206237792969\n",
      "--- 2.419502019882202 Per batch time ---\n",
      ">>>> Batch #  531 [83508, 83510, 83516, 83518, 83519, 83527, 83546, 83547, 83548, 83556, 83557, 83561, 83565, 83566, 83568, 83573, 83574, 83580, 83586, 83587, 83593, 83599, 83600, 83601, 83602, 83605, 83613, 83614, 83619, 83624, 83625, 83626]\n",
      "loss: 14.102178573608398\n",
      "--- 2.214169979095459 Per batch time ---\n",
      ">>>> Batch #  540 [84840, 84843, 84845, 84851, 84852, 84853, 84859, 84866, 84867, 84870, 84873, 84875, 84879, 84881, 84884, 84886, 84887, 84889, 84890, 84896, 84901, 84902, 84929, 84936, 84938, 84949, 84956, 84959, 84964, 84968, 84980, 84981]\n",
      "loss: 13.446207046508789\n",
      "--- 2.464862823486328 Per batch time ---\n",
      ">>>> Batch #  549 [86229, 86234, 86239, 86243, 86248, 86249, 86250, 86267, 86282, 86285, 86292, 86294, 86317, 86320, 86323, 86329, 86334, 86336, 86344, 86356, 86357, 86358, 86366, 86372, 86378, 86381, 86393, 86399, 86407, 86408, 86409, 86412]\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2767586560\n",
      "current mps driver allocated memory: 31403229184\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 18140839936\n",
      "loss: 14.295537948608398\n",
      "--- 7.1945600509643555 Per batch time ---\n",
      ">>>> Batch #  558 [87681, 87685, 87697, 87705, 87718, 87720, 87726, 87735, 87737, 87738, 87739, 87740, 87743, 87756, 87761, 87762, 87767, 87783, 87792, 87811, 87812, 87813, 87845, 87847, 87850, 87851, 87862, 87864, 87865, 87871, 87876, 87878]\n",
      "loss: 16.223316192626953\n",
      "--- 2.3357670307159424 Per batch time ---\n",
      ">>>> Batch #  567 [89238, 89248, 89253, 89254, 89258, 89259, 89266, 89268, 89273, 89281, 89285, 89288, 89293, 89298, 89318, 89320, 89322, 89329, 89340, 89350, 89355, 89356, 89359, 89362, 89367, 89369, 89378, 89384, 89390, 89391, 89395, 89397]\n",
      "loss: 10.574701309204102\n",
      "--- 2.183237314224243 Per batch time ---\n",
      ">>>> Batch #  576 [90708, 90712, 90718, 90724, 90732, 90738, 90739, 90744, 90751, 90753, 90754, 90777, 90778, 90782, 90787, 90791, 90793, 90801, 90802, 90804, 90810, 90814, 90818, 90820, 90823, 90827, 90830, 90833, 90836, 90839, 90843, 90862]\n",
      "loss: 12.260673522949219\n",
      "--- 2.0750648975372314 Per batch time ---\n",
      ">>>> Batch #  585 [92064, 92066, 92067, 92081, 92088, 92089, 92092, 92093, 92096, 92098, 92107, 92109, 92115, 92122, 92131, 92134, 92136, 92138, 92145, 92153, 92165, 92167, 92170, 92173, 92176, 92181, 92183, 92186, 92187, 92188, 92190, 92192]\n",
      "loss: 13.388813018798828\n",
      "--- 2.204439640045166 Per batch time ---\n",
      ">>>> Batch #  594 [93476, 93483, 93487, 93493, 93496, 93506, 93511, 93519, 93522, 93531, 93534, 93535, 93553, 93554, 93565, 93570, 93571, 93576, 93579, 93580, 93581, 93586, 93587, 93590, 93597, 93599, 93600, 93601, 93603, 93604, 93607, 93611]\n",
      "loss: 12.234843254089355\n",
      "--- 2.242617130279541 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2656412672\n",
      "current mps driver allocated memory: 31860408320\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 17314562048\n",
      ">>>> Batch #  603 [94793, 94795, 94807, 94813, 94817, 94823, 94825, 94826, 94828, 94837, 94841, 94842, 94846, 94858, 94859, 94863, 94865, 94877, 94884, 94885, 94890, 94893, 94920, 94922, 94925, 94926, 94936, 94940, 94946, 94949, 94951, 94952]\n",
      "loss: 13.574575424194336\n",
      "--- 2.3917927742004395 Per batch time ---\n",
      ">>>> Batch #  612 [96215, 96222, 96223, 96226, 96241, 96244, 96250, 96251, 96254, 96257, 96260, 96268, 96280, 96288, 96294, 96298, 96303, 96304, 96306, 96307, 96311, 96315, 96327, 96328, 96338, 96339, 96351, 96354, 96360, 96365, 96368, 96378]\n",
      "loss: 12.483882904052734\n",
      "--- 2.256242036819458 Per batch time ---\n",
      ">>>> Batch #  621 [97632, 97633, 97643, 97646, 97656, 97659, 97660, 97661, 97662, 97667, 97672, 97682, 97683, 97685, 97686, 97693, 97696, 97705, 97712, 97722, 97724, 97729, 97733, 97734, 97743, 97744, 97747, 97748, 97754, 97759, 97767, 97777]\n",
      "loss: 12.54179573059082\n",
      "--- 2.1434807777404785 Per batch time ---\n",
      ">>>> Batch #  630 [99040, 99041, 99046, 99062, 99063, 99064, 99065, 99066, 99067, 99070, 99073, 99077, 99081, 99086, 99094, 99099, 99104, 99108, 99112, 99115, 99119, 99129, 99135, 99139, 99159, 99162, 99165, 99170, 99177, 99179, 99180, 99184]\n",
      "loss: 14.819683074951172\n",
      "--- 2.2662720680236816 Per batch time ---\n",
      ">>>> Batch #  639 [100407, 100409, 100413, 100417, 100430, 100434, 100435, 100438, 100439, 100448, 100449, 100454, 100468, 100471, 100482, 100483, 100485, 100486, 100499, 100500, 100503, 100506, 100516, 100517, 100519, 100523, 100530, 100535, 100536, 100539, 100542, 100543]\n",
      "loss: 14.305274963378906\n",
      "--- 2.3023428916931152 Per batch time ---\n",
      ">>>> Batch #  648 [101839, 101842, 101860, 101862, 101863, 101873, 101874, 101877, 101882, 101891, 101892, 101894, 101895, 101904, 101906, 101913, 101919, 101931, 101933, 101936, 101948, 101951, 101952, 101959, 101960, 101966, 101968, 101969, 101978, 101979, 101985, 101989]\n",
      "loss: 13.752256393432617\n",
      "--- 2.4879181385040283 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2790931712\n",
      "current mps driver allocated memory: 30105092096\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 17524277248\n",
      ">>>> Batch #  657 [103306, 103307, 103315, 103318, 103320, 103331, 103332, 103335, 103338, 103341, 103348, 103352, 103355, 103356, 103358, 103361, 103366, 103375, 103379, 103380, 103383, 103386, 103390, 103393, 103401, 103403, 103404, 103413, 103414, 103419, 103427, 103428]\n",
      "loss: 14.45718002319336\n",
      "--- 2.3118598461151123 Per batch time ---\n",
      ">>>> Batch #  666 [104585, 104588, 104589, 104591, 104592, 104594, 104596, 104602, 104607, 104610, 104621, 104624, 104625, 104626, 104629, 104631, 104638, 104647, 104661, 104676, 104683, 104685, 104689, 104691, 104692, 104696, 104701, 104711, 104715, 104718, 104724, 104725]\n",
      "loss: 12.237119674682617\n",
      "--- 2.1853179931640625 Per batch time ---\n",
      ">>>> Batch #  675 [105904, 105909, 105918, 105921, 105933, 105936, 105945, 105951, 105952, 105960, 105961, 105973, 105974, 105975, 105976, 105987, 105994, 105996, 105998, 106001, 106003, 106010, 106017, 106019, 106023, 106025, 106028, 106029, 106033, 106034, 106038, 106043]\n",
      "loss: 11.533921241760254\n",
      "--- 2.293205976486206 Per batch time ---\n",
      ">>>> Batch #  684 [107304, 107305, 107306, 107313, 107331, 107340, 107346, 107351, 107353, 107357, 107360, 107362, 107367, 107375, 107376, 107384, 107386, 107389, 107394, 107402, 107405, 107421, 107424, 107425, 107427, 107428, 107430, 107436, 107438, 107443, 107450, 107454]\n",
      "loss: 13.415878295898438\n",
      "--- 2.2981886863708496 Per batch time ---\n",
      ">>>> Batch #  693 [108698, 108701, 108707, 108708, 108718, 108722, 108725, 108739, 108748, 108751, 108758, 108761, 108762, 108769, 108802, 108803, 108810, 108819, 108826, 108828, 108835, 108836, 108838, 108840, 108841, 108850, 108851, 108853, 108859, 108862, 108865, 108871]\n",
      "loss: 15.339090347290039\n",
      "--- 2.15657901763916 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2586074368\n",
      "current mps driver allocated memory: 31059296256\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 17167761408\n",
      ">>>> Batch #  702 [110094, 110105, 110108, 110111, 110123, 110136, 110138, 110142, 110155, 110156, 110157, 110159, 110169, 110170, 110174, 110175, 110187, 110196, 110200, 110203, 110204, 110209, 110217, 110230, 110231, 110233, 110240, 110250, 110251, 110252, 110257, 110258]\n",
      "loss: 15.364526748657227\n",
      "--- 2.2559940814971924 Per batch time ---\n",
      ">>>> Batch #  711 [111556, 111568, 111573, 111574, 111583, 111590, 111591, 111593, 111598, 111604, 111606, 111612, 111619, 111624, 111629, 111635, 111636, 111644, 111646, 111648, 111649, 111660, 111661, 111671, 111673, 111680, 111683, 111694, 111702, 111705, 111706, 111707]\n",
      "loss: 14.654150009155273\n",
      "--- 2.1887481212615967 Per batch time ---\n",
      ">>>> Batch #  720 [113095, 113097, 113106, 113112, 113113, 113114, 113123, 113126, 113127, 113128, 113132, 113139, 113140, 113142, 113144, 113147, 113150, 113152, 113159, 113166, 113168, 113173, 113182, 113185, 113192, 113199, 113205, 113211, 113212, 113216, 113223, 113233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 15.194068908691406\n",
      "--- 2.0799708366394043 Per batch time ---\n",
      ">>>> Batch #  729 [114479, 114481, 114500, 114503, 114504, 114505, 114510, 114515, 114519, 114521, 114535, 114540, 114541, 114549, 114559, 114561, 114579, 114586, 114598, 114616, 114617, 114624, 114629, 114630, 114634, 114648, 114652, 114653, 114657, 114661, 114668, 114673]\n",
      "loss: 15.987281799316406\n",
      "--- 2.229033946990967 Per batch time ---\n",
      ">>>> Batch #  738 [115912, 115917, 115924, 115927, 115928, 115930, 115939, 115942, 115950, 115954, 115959, 115967, 115970, 115973, 115992, 116003, 116004, 116005, 116006, 116010, 116017, 116023, 116026, 116031, 116032, 116037, 116040, 116043, 116046, 116048, 116049, 116061]\n",
      "loss: 12.29733943939209\n",
      "--- 2.161004066467285 Per batch time ---\n",
      ">>>> Batch #  747 [117317, 117319, 117321, 117322, 117325, 117327, 117328, 117336, 117337, 117349, 117352, 117360, 117366, 117368, 117371, 117377, 117379, 117380, 117389, 117396, 117400, 117403, 117404, 117407, 117413, 117417, 117418, 117424, 117428, 117432, 117441, 117445]\n",
      "loss: 12.361042022705078\n",
      "--- 2.067683219909668 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2868004096\n",
      "current mps driver allocated memory: 31420006400\n",
      "after empyt cache\n",
      "current mps allocated memory: 449792\n",
      "current mps driver allocated memory: 17673175040\n",
      ">>>> Batch #  756 [118762, 118764, 118769, 118771, 118775, 118777, 118778, 118780, 118783, 118785, 118787, 118788, 118794, 118802, 118806, 118811, 118827, 118837, 118838, 118839, 118840, 118846, 118848, 118851, 118852, 118862, 118866, 118867, 118870, 118881, 118885, 118889]\n",
      "loss: 13.770193099975586\n",
      "--- 2.4560019969940186 Per batch time ---\n",
      ">>>> Batch #  765 [120120, 120125, 120127, 120129, 120145, 120147, 120155, 120157, 120159, 120162, 120164, 120176, 120178, 120179, 120181, 120185, 120188, 120197, 120199, 120207, 120210, 120224, 120230, 120234, 120235, 120241, 120246, 120247, 120248, 120254, 120259, 120274]\n",
      "loss: 11.232512474060059\n",
      "--- 2.1713201999664307 Per batch time ---\n",
      ">>>> Batch #  774 [121622, 121632, 121633, 121644, 121647, 121649, 121651, 121659, 121661, 121663, 121665, 121666, 121676, 121677, 121678, 121682, 121683, 121690, 121692, 121693, 121706, 121709, 121716, 121718, 121720, 121731, 121745, 121748, 121749, 121754, 121762, 121769]\n",
      "loss: 15.415670394897461\n",
      "--- 2.201479196548462 Per batch time ---\n",
      ">>>> Batch #  783 [123036, 123038, 123041, 123053, 123055, 123066, 123067, 123069, 123070, 123071, 123074, 123083, 123099, 123117, 123119, 123125, 123127, 123136, 123137, 123142, 123147, 123155, 123166, 123172, 123175, 123176, 123177, 123180, 123184, 123188, 123190, 123193]\n",
      "loss: 15.25575065612793\n",
      "--- 2.262026071548462 Per batch time ---\n",
      ">>>> Batch #  792 [124609, 124614, 124615, 124617, 124620, 124621, 124622, 124627, 124629, 124643, 124647, 124652, 124654, 124663, 124664, 124684, 124694, 124703, 124707, 124709, 124711, 124712, 124715, 124718, 124723, 124728, 124729, 124731, 124732, 124734, 124736, 124737]\n",
      "loss: 12.993309020996094\n",
      "--- 2.23441219329834 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2522834944\n",
      "current mps driver allocated memory: 31113822208\n",
      "after empyt cache\n",
      "current mps allocated memory: 299776\n",
      "current mps driver allocated memory: 17220190208\n",
      ">>>> Batch #  801 [125910, 125919, 125920, 125928, 125944, 125955, 125958, 125971, 125978, 125979, 125983, 125995, 125997, 125998, 126001, 126014, 126018, 126020, 126021, 126024, 126027, 126028, 126030, 126044, 126045, 126046, 126047, 126050, 126054, 126059, 126064, 126065]\n",
      "loss: 11.96224594116211\n",
      "--- 2.4766979217529297 Per batch time ---\n",
      ">>>> Batch #  810 [127279, 127282, 127284, 127286, 127296, 127298, 127301, 127304, 127306, 127309, 127313, 127316, 127324, 127330, 127337, 127339, 127342, 127353, 127360, 127377, 127381, 127388, 127391, 127393, 127400, 127405, 127406, 127407, 127418, 127424, 127441, 127450]\n",
      "loss: 15.670269966125488\n",
      "--- 2.502530097961426 Per batch time ---\n",
      ">>>> Batch #  819 [128567, 128568, 128570, 128581, 128586, 128599, 128602, 128607, 128608, 128612, 128615, 128621, 128631, 128632, 128644, 128645, 128647, 128649, 128652, 128665, 128667, 128669, 128670, 128679, 128682, 128690, 128691, 128695, 128704, 128706, 128709, 128711]\n",
      "loss: 13.967172622680664\n",
      "--- 2.3346378803253174 Per batch time ---\n",
      ">>>> Batch #  828 [129956, 129957, 129971, 129977, 129980, 129982, 129988, 129989, 129995, 130000, 130005, 130006, 130007, 130011, 130017, 130024, 130030, 130032, 130037, 130043, 130047, 130053, 130062, 130065, 130069, 130070, 130074, 130076, 130081, 130088, 130093, 130096]\n",
      "loss: 14.133220672607422\n",
      "--- 2.281510829925537 Per batch time ---\n",
      ">>>> Batch #  837 [131416, 131418, 131419, 131427, 131434, 131449, 131450, 131453, 131460, 131465, 131467, 131470, 131485, 131486, 131487, 131490, 131493, 131494, 131497, 131498, 131504, 131509, 131511, 131516, 131522, 131524, 131527, 131531, 131533, 131534, 131539, 131547]\n",
      "loss: 14.58143424987793\n",
      "--- 2.4110870361328125 Per batch time ---\n",
      ">>>> Batch #  846 [132826, 132836, 132838, 132841, 132847, 132850, 132857, 132860, 132861, 132874, 132878, 132883, 132887, 132888, 132889, 132901, 132902, 132908, 132913, 132935, 132944, 132946, 132953, 132954, 132959, 132964, 132972, 132973, 132982, 132984, 132987, 132991]\n",
      "loss: 16.47510528564453\n",
      "--- 2.425665855407715 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2705857792\n",
      "current mps driver allocated memory: 31461949440\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 17390059520\n",
      ">>>> Batch #  855 [134268, 134271, 134278, 134285, 134286, 134288, 134290, 134294, 134297, 134302, 134306, 134309, 134320, 134324, 134325, 134327, 134328, 134334, 134339, 134343, 134344, 134346, 134351, 134362, 134366, 134375, 134378, 134383, 134386, 134389, 134396, 134413]\n",
      "loss: 12.963630676269531\n",
      "--- 2.1771719455718994 Per batch time ---\n",
      ">>>> Batch #  864 [135685, 135690, 135694, 135708, 135714, 135730, 135733, 135735, 135741, 135744, 135748, 135749, 135754, 135756, 135759, 135761, 135763, 135772, 135775, 135785, 135790, 135794, 135796, 135797, 135800, 135806, 135815, 135820, 135822, 135836, 135845, 135846]\n",
      "loss: 15.235118865966797\n",
      "--- 2.2678208351135254 Per batch time ---\n",
      ">>>> Batch #  873 [137115, 137118, 137120, 137123, 137127, 137130, 137134, 137140, 137150, 137154, 137156, 137173, 137178, 137181, 137185, 137188, 137190, 137202, 137203, 137206, 137210, 137211, 137212, 137220, 137221, 137227, 137228, 137230, 137238, 137241, 137243, 137250]\n",
      "loss: 12.245185852050781\n",
      "--- 2.1441991329193115 Per batch time ---\n",
      ">>>> Batch #  882 [138436, 138446, 138450, 138453, 138456, 138461, 138473, 138477, 138482, 138486, 138488, 138494, 138496, 138501, 138503, 138507, 138514, 138517, 138518, 138521, 138527, 138529, 138536, 138547, 138549, 138553, 138556, 138562, 138567, 138569, 138571, 138572]\n",
      "loss: 14.285612106323242\n",
      "--- 2.2923200130462646 Per batch time ---\n",
      ">>>> Batch #  891 [139748, 139749, 139754, 139757, 139758, 139763, 139775, 139780, 139781, 139782, 139785, 139787, 139789, 139796, 139811, 139815, 139817, 139831, 139836, 139839, 139843, 139856, 139858, 139865, 139873, 139878, 139887, 139889, 139892, 139897, 139904, 139907]\n",
      "loss: 14.604681015014648\n",
      "--- 2.1463911533355713 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2656410880\n",
      "current mps driver allocated memory: 31686344704\n",
      "after empyt cache\n",
      "current mps allocated memory: 327680\n",
      "current mps driver allocated memory: 17249550336\n",
      ">>>> Batch #  900 [141160, 141163, 141166, 141172, 141180, 141181, 141197, 141200, 141201, 141204, 141205, 141207, 141211, 141219, 141224, 141228, 141229, 141232, 141236, 141240, 141247, 141251, 141256, 141257, 141263, 141265, 141271, 141278, 141283, 141302, 141304, 141316]\n",
      "loss: 13.247385025024414\n",
      "--- 2.296320676803589 Per batch time ---\n",
      ">>>> Batch #  909 [142660, 142665, 142666, 142667, 142668, 142672, 142674, 142680, 142684, 142686, 142687, 142689, 142697, 142698, 142711, 142712, 142718, 142719, 142722, 142726, 142733, 142735, 142741, 142742, 142744, 142746, 142749, 142757, 142761, 142767, 142769, 142771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.108471870422363\n",
      "--- 2.268552780151367 Per batch time ---\n",
      ">>>> Batch #  918 [143995, 144000, 144007, 144018, 144022, 144025, 144027, 144033, 144036, 144049, 144053, 144056, 144058, 144062, 144064, 144065, 144079, 144084, 144088, 144089, 144091, 144093, 144115, 144122, 144130, 144139, 144147, 144157, 144161, 144162, 144163, 144167]\n",
      "loss: 13.956521034240723\n",
      "--- 2.1186130046844482 Per batch time ---\n",
      ">>>> Batch #  927 [145385, 145391, 145405, 145408, 145409, 145411, 145422, 145426, 145429, 145432, 145436, 145439, 145444, 145447, 145448, 145452, 145460, 145462, 145476, 145479, 145480, 145488, 145497, 145499, 145502, 145503, 145512, 145518, 145520, 145523, 145526, 145528]\n",
      "loss: 9.847282409667969\n",
      "--- 2.161724090576172 Per batch time ---\n",
      ">>>> Batch #  936 [146855, 146857, 146861, 146865, 146871, 146878, 146881, 146885, 146886, 146887, 146901, 146907, 146910, 146917, 146926, 146929, 146933, 146936, 146951, 146954, 146959, 146961, 146963, 146965, 146972, 146973, 146979, 146981, 146986, 146988, 146997, 146999]\n",
      "loss: 8.882078170776367\n",
      "--- 1.9908459186553955 Per batch time ---\n",
      ">>>> Batch #  945 [148148, 148165, 148168, 148170, 148176, 148181, 148184, 148188, 148193, 148196, 148197, 148205, 148206, 148208, 148215, 148217, 148229, 148230, 148239, 148240, 148243, 148246, 148251, 148263, 148267, 148272, 148274, 148276, 148280, 148282, 148286, 148291]\n",
      "loss: 13.632099151611328\n",
      "--- 2.285806179046631 Per batch time ---\n",
      "=========== mps sync, gc, and mps empty cache and reload ==========\n",
      "current mps allocated memory: 2535258880\n",
      "current mps driver allocated memory: 31141085184\n",
      "after empyt cache\n",
      "current mps allocated memory: 299776\n",
      "current mps driver allocated memory: 17241161728\n",
      ">>>> Batch #  954 [149588, 149592, 149598, 149602, 149604, 149610, 149615, 149616, 149623, 149630, 149634, 149641, 149643, 149645, 149646, 149648, 149657, 149660, 149668, 149669, 149676, 149679, 149682, 149686, 149712, 149726, 149731, 149737, 149738, 149739, 149755, 149763]\n",
      "loss: 15.047473907470703\n",
      "--- 2.2515647411346436 Per batch time ---\n",
      ">>>> FAILED! Batch #  959 [150435, 150440, 150442, 150444, 150455, 150462, 150473, 150477, 150480, 150487, 150495, 150500, 150502, 150508, 150512, 150533, 150537, 150538, 150541, 150546, 150552, 150558, 150559, 150562, 150576, 150594, 150599, 150614, 150616, 150623, 150636, 150639]\n",
      "current mps allocated memory: 14340513024\n",
      "current mps driver allocated memory: 37740822528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v6/jl9x0j851bv51t8wm91_943c0000gn/T/ipykernel_69274/1882192884.py\", line 97, in training\n",
      "    loss = do_train(model, optimizer, idx, x, target, should_print).detach()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v6/jl9x0j851bv51t8wm91_943c0000gn/T/ipykernel_69274/1882192884.py\", line 62, in do_train\n",
      "    loss.backward()\n",
      "  File \"/Users/xiangyuliu/sources/pytorch/torch/_tensor.py\", line 523, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/xiangyuliu/sources/pytorch/torch/autograd/__init__.py\", line 284, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/Users/xiangyuliu/sources/pytorch/torch/autograd/graph.py\", line 767, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 17.16 GB, other allocations: 17.99 GB, max allowed: 36.27 GB). Tried to allocate 2.22 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "run_name = \"with_sync_\" + current_time\n",
    "writer = SummaryWriter(Path.joinpath(constants.TB_OUT_PATH, run_name))\n",
    "\n",
    "#model = training(run_name, writer, 40, sync_after_every_n=2, print_every = 1,  early_terminate = 10)\n",
    "model = training(run_name, writer, 3, sync_after_every_n=50, print_every = 10,  early_terminate = None)\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f67978",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "# writer = SummaryWriter(Path.joinpath(constants.TB_OUT_PATH, current_time))\n",
    "\n",
    "# #training(model, writer, 5, empty_catch_after_every_n=None, early_terminate = 10)\n",
    "# model = training(writer, 5, sync_after_every_n=None, print_every = 1, early_terminate = 10)\n",
    "# #training(model, writer, 2, empty_catch_after_every_n=None)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b96f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manual(dataset, size):\n",
    "    items = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        item = dataset.__getitem__(i)\n",
    "        # replace the `qa` with just the `qs`\n",
    "        print(item.annotations['qa'])\n",
    "        item.annotations['qa'] = item.annotations['qs']\n",
    "        items.append(item)\n",
    "    return collate_fn2(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e6504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\">>> original qa\")\n",
    "test1x, target = manual(val, 5)\n",
    "model, optimizer = reload_model(0.01, \"train-with_sync_Jun02_01-22-14-epoch-1-batch-7\")\n",
    "model.eval()\n",
    "answers = model.answer(test1x, device, max_length = 30)\n",
    "\n",
    "print(\">>> prediction\")\n",
    "def token_to_word(x):\n",
    "    qa = x[\"qa\"]\n",
    "    return tokenizer.batch_decode(qa)\n",
    "\n",
    "token_to_word(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3232d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_answers(x):\n",
    "    qa = x[\"qa\"]\n",
    "    answer_token_id = tokenizer.convert_tokens_to_ids(constants.ANSWER_TOKEN)\n",
    "\n",
    "    answer_start =  (qa == answer_token_id).nonzero()\n",
    "    mask = torch.zeros_like(qa)\n",
    "    mask[answer_start[:, 0], answer_start[:, 1]] = 1\n",
    "    # fill the elements after the [ANSWER] token to be 1.\n",
    "    mask = mask.cumsum(dim=1)\n",
    "    just_answers = qa * mask\n",
    "    return tokenizer.batch_decode(just_answers, skip_special_tokens = True)\n",
    "    \n",
    "real_answers = get_answers(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(x, answers, result):\n",
    "    qids = x['qids']\n",
    "    assert len(qids) == len(answers)\n",
    "    for i in range(len(qids)):\n",
    "        d = {\"question_id\" : qids[i], \"answer\": answers[i]}\n",
    "        result.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e711dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "create_output(test1x, real_answers, result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12acc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.questions.loc[val.questions['question_id'] == 139001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523df73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
